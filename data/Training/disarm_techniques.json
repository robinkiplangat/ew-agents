{
    "influence_operation_context": {
      "meta_narratives": [
        {
          "meta_narrative": "Foreign Interference / Neocolonialism",
          "description": "Claims that Western nations manipulate elections.",
          "tactics_used": [
            "Fake reports",
            "NGO conspiracies",
            "doctored foreign news"
          ],
          "common_locations": [
            "Mali",
            "Burkina Faso",
            "Senegal",
            "Niger"
          ],
          "purpose_effect": "Delegitimise pro-West candidates; justify juntas."
        },
        {
          "meta_narrative": "Rigged Elections / Stolen Mandate",
          "description": "Accusations of widespread rigging by incumbents.",
          "tactics_used": [
            "Fake tally sheets",
            "videos of \"ballot stuffing\"",
            "pre-election rigging claims"
          ],
          "common_locations": [
            "Nigeria",
            "Ghana",
            "Sierra Leone"
          ],
          "purpose_effect": "Undermine confidence in the electoral process."
        },
        {
          "meta_narrative": "Tech Manipulation Narratives",
          "description": "Suggests that biometric systems or voting tech are compromised.",
          "tactics_used": [
            "Fake tech \"hacks\"",
            "rumours about foreign-manufactured voting machines"
          ],
          "common_locations": [
            "Nigeria (BVAS)",
            "Kenya"
          ],
          "purpose_effect": "Undermine trust in digitised election systems."
        },
        {
          "meta_narrative": "Populist \"Clean Outsider\" Narratives",
          "description": "Frame outsider candidates as incorruptible, saviour figures.",
          "tactics_used": [
            "Exaggerated life stories",
            "viral infographics",
            "inspirational montages"
          ],
          "common_locations": [
            "Senegal (Pastef)",
            "Nigeria (Obidients)"
          ],
          "purpose_effect": "Fuel cult-like devotion; position candidate as anti-establishment messiah."
        },
        {
          "meta_narrative": "Divine / Prophetic Endorsements",
          "description": "Candidates are presented as \"chosen by God\" or predicted to win by religious figures.",
          "tactics_used": [
            "Viral prophecy videos",
            "fake endorsements from religious leaders"
          ],
          "common_locations": [
            "Nigeria",
            "Ghana"
          ],
          "purpose_effect": "Exploit religious beliefs for legitimacy or support."
        },
        {
          "meta_narrative": "Ethnic / Religious Polarisation",
          "description": "Amplifies ethnic or religious divisions for political gain.",
          "tactics_used": [
            "Hate speech",
            "fake news targeting ethnic groups",
            "fabricated security threats"
          ],
          "common_locations": [
            "Nigeria",
            "Kenya",
            "Ethiopia"
          ],
          "purpose_effect": "Incite violence, suppress votes, polarise the electorate."
        },
        {
          "meta_narrative": "Youth Disenfranchisement Claims",
          "description": "Claims that the system is rigged against young voters.",
          "tactics_used": [
            "Viral posts about PVC collection issues",
            "fake INEC announcements"
          ],
          "common_locations": [
            "Nigeria",
            "Sierra Leone"
          ],
          "purpose_effect": "Suppress youth voter turnout, create apathy."
        },
        {
          "meta_narrative": "Coup or Military Narratives",
          "description": "Spreads rumours of an impending military coup or intervention.",
          "tactics_used": [
            "Fake military statements",
            "manipulated videos of troop movements"
          ],
          "common_locations": [
            "Burkina Faso",
            "Mali",
            "Sudan"
          ],
          "purpose_effect": "Create instability, undermine civilian rule."
        },
        {
          "meta_narrative": "Fake Polls and Media Clones",
          "description": "Fabricates polling data or clones legitimate news sites to spread disinformation.",
          "tactics_used": [
            "Professional-looking graphics with fake poll results",
            "imposter news sites"
          ],
          "common_locations": [
            "Kenya",
            "Nigeria"
          ],
          "purpose_effect": "Manipulate public opinion, create a bandwagon effect."
        }
      ],
      "disarm_techniques_and_tactics": [
        {
          "disarm_id": "T0002",
          "name": "Facilitate State Propaganda",
          "tactic_id": "TA02",
          "summary": "Organise citizens around pro-state messaging. Coordinate paid or volunteer groups to push state propaganda.",
          "changes_from_v0_1": "no change",
          "long_name": "T0002 - Facilitate State Propaganda",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0003",
          "name": "Leverage Existing Narratives",
          "tactic_id": "TA14",
          "summary": "Use or adapt existing narrative themes, where narratives are the baseline stories of a target audience. Narratives form the bedrock of our worldviews. New information is understood through a process firmly grounded in this bedrock. If new information is not consitent with the prevailing narratives of an audience, it will be ignored. Effective campaigns will frame their misinformation in the context of these narratives. Highly effective campaigns will make extensive use of audience-appropriate archetypes and meta-narratives throughout their content creation and amplifiction practices.",
          "changes_from_v0_1": "no change",
          "long_name": "T0003 - Leverage Existing Narratives",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0004",
          "name": "Develop Competing Narratives",
          "tactic_id": "TA14",
          "summary": "Advance competing narratives connected to same issue ie: on one hand deny incident while at same time expresses dismiss. Suppressing or discouraging narratives already spreading requires an alternative. The most simple set of narrative techniques in response would be the construction and promotion of contradictory alternatives centred on denial, deflection, dismissal, counter-charges, excessive standards of proof, bias in prohibition or enforcement, and so on. These competing narratives allow loyalists cover, but are less compelling to opponents and fence-sitters than campaigns built around existing narratives or highly explanatory master narratives. Competing narratives, as such, are especially useful in the \"firehose of misinformation\" approach.",
          "changes_from_v0_1": "no change",
          "long_name": "T0004 - Develop Competing Narratives",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0010",
          "name": "Cultivate Ignorant Agents",
          "tactic_id": "TA15",
          "summary": "Cultivate propagandists for a cause, the goals of which are not fully comprehended, and who are used cynically by the leaders of the cause. Independent actors use social media and specialised web sites to strategically reinforce and spread messages compatible with their own. Their networks are infiltrated and used by state media disinformation organisations to amplify the state’s own disinformation strategies against target populations. Many are traffickers in conspiracy theories or hoaxes, unified by a suspicion of Western governments and mainstream media. Their narratives, which appeal to leftists hostile to globalism and military intervention and nationalists against immigration, are frequently infiltrated and shaped by state-controlled trolls and altered news items from agencies such as RT and Sputnik. Also know as \"useful idiots\" or \"unwitting agents\".",
          "changes_from_v0_1": "no change",
          "long_name": "T0010 - Cultivate Ignorant Agents",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0014",
          "name": "Prepare Fundraising Campaigns",
          "tactic_id": "TA15",
          "summary": "Fundraising campaigns refer to an influence operation’s systematic effort to seek financial support for a charity, cause, or other enterprise using online activities that further promote operation information pathways while raising a profit. Many influence operations have engaged in crowdfunding services on platforms including Tipee, Patreon, and GoFundMe. An operation may use its previously prepared fundraising campaigns (see: Develop Information Pathways) to promote operation messaging while raising money to support its activities.",
          "changes_from_v0_1": "no change",
          "long_name": "T0014 - Prepare Fundraising Campaigns",
          "sub_techniques": [
            {
              "disarm_id": "T0014.001",
              "name": "Raise Funds from Malign Actors",
              "tactic_id": "TA15",
              "summary": "Raising funds from malign actors may include contributions from foreign agents, cutouts or proxies, shell companies, dark money groups, etc.",
              "changes_from_v0_1": "no change",
              "long_name": "Raise Funds from Malign Actors -"
            },
            {
              "disarm_id": "T0014.002",
              "name": "Raise Funds from Ignorant Agents",
              "tactic_id": "TA15",
              "summary": "Raising funds from ignorant agents may include scams, donations intended for one stated purpose but then used for another, etc.",
              "changes_from_v0_1": "no change",
              "long_name": "Raise Funds from Ignorant Agents -"
            }
          ]
        },
        {
          "disarm_id": "T0015",
          "name": "Create Hashtags and Search Artefacts",
          "tactic_id": "TA06",
          "summary": "Create one or more hashtags and/or hashtag groups. Many incident-based campaigns will create hashtags to promote their fabricated event. Creating a hashtag for an incident can have two important effects: 1. Create a perception of reality around an event. Certainly only \"real\" events would be discussed in a hashtag. After all, the event has a name!, and 2. Publicise the story more widely through trending lists and search behaviour. Asset needed to direct/control/manage \"conversation\" connected to launching new incident/campaign with new hashtag for applicable social media sites).",
          "changes_from_v0_1": "no change",
          "long_name": "T0015 - Create Hashtags and Search Artefacts",
          "sub_techniques": [
            {
              "disarm_id": "T0015.001",
              "name": "Use Existing Hashtag",
              "tactic_id": "TA06",
              "summary": "Use a dedicated, existing hashtag for the campaign/incident. This Technique covers behaviours previously documented by T0104.005: Use Hashtags, which has since been deprecated.  ",
              "changes_from_v0_1": "moved in V1.6",
              "long_name": "Use Existing Hashtag -"
            },
            {
              "disarm_id": "T0015.002",
              "name": "Create New Hashtag",
              "tactic_id": "TA06",
              "summary": "Create a campaign/incident specific hashtag. This Technique covers behaviours previously documented by T0104.006: Create Dedicated Hashtag, which has since been deprecated. ",
              "changes_from_v0_1": "moved in V1.6",
              "long_name": "Create New Hashtag -"
            }
          ]
        },
        {
          "disarm_id": "T0016",
          "name": "Create Clickbait",
          "tactic_id": "TA05",
          "summary": "Create attention grabbing headlines (outrage, doubt, humour) required to drive traffic & engagement. This is a key asset.",
          "changes_from_v0_1": "no change",
          "long_name": "T0016 - Create Clickbait",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0017",
          "name": "Conduct Fundraising",
          "tactic_id": "TA10",
          "summary": "Fundraising campaigns refer to an influence operation’s systematic effort to seek financial support for a charity, cause, or other enterprise using online activities that further promote operation information pathways while raising a profit. Many influence operations have engaged in crowdfunding services166 on platforms including Tipee, Patreon, and GoFundMe. An operation may use its previously prepared fundraising campaigns to promote operation messaging while raising money to support its activities.",
          "changes_from_v0_1": "no change",
          "long_name": "T0017 - Conduct Fundraising",
          "sub_techniques": [
            {
              "disarm_id": "T0017.001",
              "name": "Conduct Crowdfunding Campaigns",
              "tactic_id": "TA10",
              "summary": "An influence operation may Conduct Crowdfunding Campaigns on platforms such as GoFundMe, GiveSendGo, Tipeee, Patreon, etc.",
              "changes_from_v0_1": "no change",
              "long_name": "Conduct Crowdfunding Campaigns -"
            }
          ]
        },
        {
          "disarm_id": "T0018",
          "name": "Purchase Targeted Advertisements",
          "tactic_id": "TA05",
          "summary": "Create or fund advertisements targeted at specific populations,",
          "changes_from_v0_1": "no change",
          "long_name": "T0018 - Purchase Targeted Advertisements",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0020",
          "name": "Trial Content",
          "tactic_id": "TA08",
          "summary": "Iteratively test incident performance (messages, content etc), e.g. A/B test headline/content enagagement metrics; website and/or funding campaign conversion rates",
          "changes_from_v0_1": "no change",
          "long_name": "T0020 - Trial Content",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0022",
          "name": "Leverage Conspiracy Theory Narratives",
          "tactic_id": "TA14",
          "summary": "\"Conspiracy narratives\" appeal to the human desire for explanatory order, by invoking the participation of poweful (often sinister) actors in pursuit of their own political goals. These narratives are especially appealing when an audience is low-information, marginalised or otherwise inclined to reject the prevailing explanation. Conspiracy narratives are an important component of the \"firehose of falsehoods\" model.",
          "changes_from_v0_1": "no change",
          "long_name": "T0022 - Leverage Conspiracy Theory Narratives",
          "sub_techniques": [
            {
              "disarm_id": "T0022.001",
              "name": "Amplify Existing Conspiracy Theory Narratives",
              "tactic_id": "TA14",
              "summary": "An influence operation may amplify an existing conspiracy theory narrative that aligns with its incident or campaign goals. By amplifying existing conspiracy theory narratives, operators can leverage the power of the existing communities that support and propagate those theories without needing to expend resources creating new narratives or building momentum and buy in around new narratives.",
              "changes_from_v0_1": "no change",
              "long_name": "Amplify Existing Conspiracy Theory Narratives -"
            },
            {
              "disarm_id": "T0022.002",
              "name": "Develop Original Conspiracy Theory Narratives",
              "tactic_id": "TA14",
              "summary": "While this requires more resources than amplifying existing conspiracy theory narratives, an influence operation may develop original conspiracy theory narratives in order to achieve greater control and alignment over the narrative and their campaign goals. Prominent examples include the USSR's Operation INFEKTION disinformation campaign run by the KGB in the 1980s to plant the idea that the United States had invented HIV/AIDS as part of a biological weapons research project at Fort Detrick, Maryland. More recently, Fort Detrick featured prominently in a new conspiracy theory narratives around the origins of the COVID-19 outbreak and pandemic.",
              "changes_from_v0_1": "no change",
              "long_name": "Develop Original Conspiracy Theory Narratives -"
            }
          ]
        },
        {
          "disarm_id": "T0023",
          "name": "Distort Facts",
          "tactic_id": "TA06",
          "summary": "Change, twist, or exaggerate existing facts to construct a narrative that differs from reality. Examples: images and ideas can be distorted by being placed in an improper content",
          "changes_from_v0_1": "no change",
          "long_name": "T0023 - Distort Facts",
          "sub_techniques": [
            {
              "disarm_id": "T0023.001",
              "name": "Reframe Context",
              "tactic_id": "TA06",
              "summary": "Reframing context refers to removing an event from its surrounding context to distort its intended meaning. Rather than deny that an event occurred, reframing context frames an event in a manner that may lead the target audience to draw a different conclusion about its intentions.",
              "changes_from_v0_1": "no change",
              "long_name": "Reframe Context -"
            },
            {
              "disarm_id": "T0023.002",
              "name": "Edit Open-Source Content",
              "tactic_id": "TA06",
              "summary": "An influence operation may edit open-source content, such as collaborative blogs or encyclopaedias, to promote its narratives on outlets with existing credibility and audiences. Editing open-source content may allow an operation to post content on platforms without dedicating resources to the creation and maintenance of its own assets.",
              "changes_from_v0_1": "no change",
              "long_name": "Edit Open-Source Content -"
            }
          ]
        },
        {
          "disarm_id": "T0029",
          "name": "Online Polls",
          "tactic_id": "TA07",
          "summary": "Create fake online polls, or manipulate existing online polls. Data gathering tactic to target those who engage, and potentially their networks of friends/followers as well",
          "changes_from_v0_1": "no change",
          "long_name": "T0029 - Online Polls",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0039",
          "name": "Bait Influencer",
          "tactic_id": "TA17",
          "summary": "Influencers are people on social media platforms who have large audiences.\n \nThreat Actors can try to trick Influencers such as celebrities, journalists, or local leaders who aren’t associated with their campaign into amplifying campaign content. This gives them access to the Influencer’s audience without having to go through the effort of building it themselves, and it helps legitimise their message by associating it with the Influencer, benefitting from their audience’s trust in them.",
          "changes_from_v0_1": "New Summary, Tactic, V1.4",
          "long_name": "T0039 - Bait Influencer",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0040",
          "name": "Demand Insurmountable Proof",
          "tactic_id": "TA14",
          "summary": "Campaigns often leverage tactical and informational asymmetries on the threat surface, as seen in the Distort and Deny strategies, and the \"firehose of misinformation\". Specifically, conspiracy theorists can be repeatedly wrong, but advocates of the truth need to be perfect. By constantly escalating demands for proof, propagandists can effectively leverage this asymmetry while also priming its future use, often with an even greater asymmetric advantage. The conspiracist is offered freer rein for a broader range of \"questions\" while the truth teller is burdened with higher and higher standards of proof.",
          "changes_from_v0_1": "no change",
          "long_name": "T0040 - Demand Insurmountable Proof",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0042",
          "name": "Seed Kernel of Truth",
          "tactic_id": "TA08",
          "summary": "Wrap lies or altered context/facts around truths. Influence campaigns pursue a variety of objectives with respect to target audiences, prominent among them: 1. undermine a narrative commonly referenced in the target audience; or 2. promote a narrative less common in the target audience, but preferred by the attacker. In both cases, the attacker is presented with a heavy lift. They must change the relative importance of various narratives in the interpretation of events, despite contrary tendencies. When messaging makes use of factual reporting to promote these adjustments in the narrative space, they are less likely to be dismissed out of hand; when messaging can juxtapose a (factual) truth about current affairs with the (abstract) truth explicated in these narratives, propagandists can undermine or promote them selectively. Context matters.",
          "changes_from_v0_1": "no change",
          "long_name": "T0042 - Seed Kernel of Truth",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0044",
          "name": "Seed Distortions",
          "tactic_id": "TA08",
          "summary": "Try a wide variety of messages in the early hours surrounding an incident or event, to give a misleading account or impression.",
          "changes_from_v0_1": "no change",
          "long_name": "T0044 - Seed Distortions",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0045",
          "name": "Use Fake Experts",
          "tactic_id": "TA08",
          "summary": "Use the fake experts that were set up during Establish Legitimacy. Pseudo-experts are disposable assets that often appear once and then disappear. Give \"credility\" to misinformation. Take advantage of credential bias",
          "changes_from_v0_1": "no change",
          "long_name": "T0045 - Use Fake Experts",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0046",
          "name": "Use Search Engine Optimisation",
          "tactic_id": "TA08",
          "summary": "Manipulate content engagement metrics (ie: Reddit & Twitter) to influence/impact news search results (e.g. Google), also elevates RT & Sputnik headline into Google news alert emails. aka \"Black-hat SEO\"",
          "changes_from_v0_1": "no change",
          "long_name": "T0046 - Use Search Engine Optimisation",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0047",
          "name": "Censor Social Media as a Political Force",
          "tactic_id": "TA18",
          "summary": "Use political influence or the power of state to stop critical social media comments. Government requested/driven content take downs (see Google Transperancy reports).",
          "changes_from_v0_1": "no change",
          "long_name": "T0047 - Censor Social Media as a Political Force",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0048",
          "name": "Harass",
          "tactic_id": "TA18",
          "summary": "Threatening or harassing believers of opposing narratives refers to the use of intimidation techniques, including cyberbullying and doxing, to discourage opponents from voicing their dissent. An influence operation may threaten or harass believers of the opposing narratives to deter individuals from posting or proliferating conflicting content.",
          "changes_from_v0_1": "no change",
          "long_name": "T0048 - Harass",
          "sub_techniques": [
            {
              "disarm_id": "T0048.001",
              "name": "Boycott/\"Cancel\" Opponents",
              "tactic_id": "TA18",
              "summary": "Cancel culture refers to the phenomenon in which individuals collectively refrain from supporting an individual, organisation, business, or other entity, usually following a real or falsified controversy. An influence operation may exploit cancel culture by emphasising an adversary’s problematic or disputed behaviour and presenting its own content as an alternative.",
              "changes_from_v0_1": "no change",
              "long_name": "Boycott/\"Cancel\" Opponents -"
            },
            {
              "disarm_id": "T0048.002",
              "name": "Harass People Based on Identities",
              "tactic_id": "TA18",
              "summary": "Examples include social identities like gender, sexuality, race, ethnicity, religion, ability, nationality, etc. as well as roles and occupations like journalist or activist.",
              "changes_from_v0_1": "no change",
              "long_name": "Harass People Based on Identities -"
            },
            {
              "disarm_id": "T0048.003",
              "name": "Threaten to Dox",
              "tactic_id": "TA18",
              "summary": "Doxing refers to online harassment in which individuals publicly release private information about another individual, including names, addresses, employment information, pictures, family members, and other sensitive information. An influence operation may dox its opposition to encourage individuals aligned with operation narratives to harass the doxed individuals themselves or otherwise discourage the doxed individuals from posting or proliferating conflicting content.",
              "changes_from_v0_1": "no change",
              "long_name": "Threaten to Dox -"
            },
            {
              "disarm_id": "T0048.004",
              "name": "Dox",
              "tactic_id": "TA18",
              "summary": "Doxing refers to online harassment in which individuals publicly release private information about another individual, including names, addresses, employment information, pictures, family members, and other sensitive information. An influence operation may dox its opposition to encourage individuals aligned with operation narratives to harass the doxed individuals themselves or otherwise discourage the doxed individuals from posting or proliferating conflicting content.",
              "changes_from_v0_1": "no change",
              "long_name": "Dox -"
            }
          ]
        },
        {
          "disarm_id": "T0049",
          "name": "Flood Information Space",
          "tactic_id": "TA17",
          "summary": "Flooding sources of information (e.g. Social Media feeds) with a high volume of inauthentic content.\n\nThis can be done to control/shape online conversations, drown out opposing points of view, or make it harder to find legitimate information.\n\nBots and/or patriotic trolls are effective tools to achieve this effect.\n\nThis Technique previously used the name Flooding the Information Space.",
          "changes_from_v0_1": "Updated V1.4",
          "long_name": "T0049 - Flood Information Space",
          "sub_techniques": [
            {
              "disarm_id": "T0049.001",
              "name": "Trolls Amplify and Manipulate",
              "tactic_id": "TA17",
              "summary": "Use trolls to amplify narratives and/or manipulate narratives. Fake profiles/sockpuppets operating to support individuals/narratives from the entire political spectrum (left/right binary). Operating with increased emphasis on promoting local content and promoting real Twitter users generating their own, often divisive political content, as it's easier to amplify existing content than create new/original content. Trolls operate where ever there's a socially divisive issue (issues that can/are be politicized).",
              "changes_from_v0_1": "no change",
              "long_name": "Trolls Amplify and Manipulate -"
            },
            {
              "disarm_id": "T0049.002",
              "name": "Flood Existing Hashtag",
              "tactic_id": "TA17",
              "summary": "Hashtags can be used by communities to collate information they post about particular topics (such as their interests, or current events) and users can find communities to join by exploring hashtags they’re interested in.\n\nThreat actors can flood an existing hashtag to try to ruin hashtag functionality, posting content unrelated to the hashtag alongside it, making it a less reliable source of relevant information. They may also try to flood existing hashtags with campaign content, with the intent of maximising exposure to users.\n\nThis Technique covers cases where threat actors flood existing hashtags with campaign content.\n\nThis Technique covers behaviours previously documented by T0019.002: Hijack Hashtags, which has since been deprecated. This Technique was previously called Hijack Existing Hashtag.",
              "changes_from_v0_1": "Updated V1.4",
              "long_name": "Hijack Existing Hashtag -"
            },
            {
              "disarm_id": "T0049.003",
              "name": "Bots Amplify via Automated Forwarding and Reposting",
              "tactic_id": "TA17",
              "summary": "Automated forwarding and reposting refer to the proliferation of operation content using automated means, such as artificial intelligence or social media bots. An influence operation may use automated activity to increase content exposure without dedicating the resources, including personnel and time, traditionally required to forward and repost content. Use bots to amplify narratives above algorithm thresholds. Bots are automated/programmed profiles designed to amplify content (ie: automatically retweet or like) and give appearance it's more \"popular\" than it is. They can operate as a network, to function in a coordinated/orchestrated manner. In some cases (more so now) they are an inexpensive/disposable assets used for minimal deployment as bot detection tools improve and platforms are more responsive.",
              "changes_from_v0_1": "no change",
              "long_name": "Bots Amplify via Automated Forwarding and Reposting -"
            },
            {
              "disarm_id": "T0049.004",
              "name": "Utilise Spamoflauge",
              "tactic_id": "TA17",
              "summary": "Spamoflauge refers to the practice of disguising spam messages as legitimate. Spam refers to the use of electronic messaging systems to send out unrequested or unwanted messages in bulk. Simple methods of spamoflauge include replacing letters with numbers to fool keyword-based email spam filters, for example, \"you've w0n our jackp0t!\". Spamoflauge may extend to more complex techniques such as modifying the grammar or word choice of the language, casting messages as images which spam detectors cannot automatically read, or encapsulating messages in password protected attachments, such as .pdf or .zip files. Influence operations may use spamoflauge to avoid spam filtering systems and increase the likelihood of the target audience receiving operation messaging.",
              "changes_from_v0_1": "no change",
              "long_name": "Utilise Spamoflauge -"
            },
            {
              "disarm_id": "T0049.005",
              "name": "Conduct Swarming",
              "tactic_id": "TA17",
              "summary": "Swarming refers to the coordinated use of accounts to overwhelm the information space with operation content. Unlike information flooding, swarming centres exclusively around a specific event or actor rather than a general narrative. Swarming relies on “horizontal communication” between information assets rather than a top-down, vertical command-and-control approach.",
              "changes_from_v0_1": "no change",
              "long_name": "Conduct Swarming -"
            },
            {
              "disarm_id": "T0049.006",
              "name": "Conduct Keyword Squatting",
              "tactic_id": "TA17",
              "summary": "Keyword squatting refers to the creation of online content, such as websites, articles, or social media accounts, around a specific search engine-optimized term to overwhelm the search results of that term. An influence may keyword squat to increase content exposure to target audience members who query the exploited term in a search engine and manipulate the narrative around the term.",
              "changes_from_v0_1": "no change",
              "long_name": "Conduct Keyword Squatting -"
            },
            {
              "disarm_id": "T0049.007",
              "name": "Inauthentic Sites Amplify News and Narratives",
              "tactic_id": "TA17",
              "summary": "Inauthentic sites circulate cross-post stories and amplify narratives. Often these sites have no masthead, bylines or attribution.",
              "changes_from_v0_1": "no change",
              "long_name": "Inauthentic Sites Amplify News and Narratives -"
            },
            {
              "disarm_id": "T0049.008",
              "name": "Generate Information Pollution",
              "tactic_id": "TA17",
              "summary": "Information Pollution occurs when threat actors attempt to ruin a source of information by flooding it with lots of inauthentic or unreliable content, intending to make it harder for legitimate users to find the information they’re looking for.\n \nThis sub-technique’s objective is to reduce exposure to target information, rather than promoting exposure to campaign content, for which the parent Technique T0049 can be used.\n \nAnalysts will need to infer what the motive for flooding an information space was when deciding whether to use T0049 or T0049.008 to tag a case when an information space is flooded. If such inference is not possible, default to T0049.\n \nThis Technique previously used the ID T0019.",
              "changes_from_v0_1": "Added V1.4",
              "long_name": ""
            }
          ]
        },
        {
          "disarm_id": "T0057",
          "name": "Organise Events",
          "tactic_id": "TA10",
          "summary": "Coordinate and promote real-world events across media platforms, e.g. rallies, protests, gatherings in support of incident narratives.",
          "changes_from_v0_1": "no change",
          "long_name": "T0057 - Organise Events",
          "sub_techniques": [
            {
              "disarm_id": "T0057.001",
              "name": "Pay for Physical Action",
              "tactic_id": "TA10",
              "summary": "Paying for physical action occurs when an influence operation pays individuals to act in the physical realm. An influence operation may pay for physical action to create specific situations and frame them in a way that supports operation narratives, for example, paying a group of people to burn a car to later post an image of the burning car and frame it as an act of protest.",
              "changes_from_v0_1": "no change",
              "long_name": "Pay for Physical Action -"
            },
            {
              "disarm_id": "T0057.002",
              "name": "Conduct Symbolic Action",
              "tactic_id": "TA10",
              "summary": "Symbolic action refers to activities specifically intended to advance an operation’s narrative by signalling something to the audience, for example, a military parade supporting a state’s narrative of military superiority. An influence operation may use symbolic action to create falsified evidence supporting operation narratives in the physical information space.",
              "changes_from_v0_1": "no change",
              "long_name": "Conduct Symbolic Action -"
            }
          ]
        },
        {
          "disarm_id": "T0059",
          "name": "Play the Long Game",
          "tactic_id": "TA11",
          "summary": "Play the long game refers to two phenomena: 1. To plan messaging and allow it to grow organically without conducting your own amplification. This is methodical and slow and requires years for the message to take hold 2. To develop a series of seemingly disconnected messaging narratives that eventually combine into a new narrative.",
          "changes_from_v0_1": "no change",
          "long_name": "T0059 - Play the Long Game",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0060",
          "name": "Continue to Amplify",
          "tactic_id": "TA11",
          "summary": "continue narrative or message amplification after the main incident work has finished",
          "changes_from_v0_1": "no change",
          "long_name": "T0060 - Continue to Amplify",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0061",
          "name": "Sell Merchandise",
          "tactic_id": "TA10",
          "summary": "Sell mechandise refers to getting the message or narrative into physical space in the offline world while making money",
          "changes_from_v0_1": "no change",
          "long_name": "T0061 - Sell Merchandise",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0065",
          "name": "Prepare Physical Broadcast Capabilities",
          "tactic_id": "TA15",
          "summary": "Create or coopt broadcast capabilities (e.g. TV, radio etc).",
          "changes_from_v0_1": "no change",
          "long_name": "T0065 - Prepare Physical Broadcast Capabilities",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0066",
          "name": "Degrade Adversary",
          "tactic_id": "TA02",
          "summary": "Plan to degrade an adversary’s image or ability to act. This could include preparation and use of harmful information about the adversary’s actions or reputation.",
          "changes_from_v0_1": "no change",
          "long_name": "T0066 - Degrade Adversary",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0068",
          "name": "Respond to Breaking News Event or Active Crisis",
          "tactic_id": "TA14",
          "summary": "Media attention on a story or event is heightened during a breaking news event, where unclear facts and incomplete information increase speculation, rumours, and conspiracy theories, which are all vulnerable to manipulation.",
          "changes_from_v0_1": "no change",
          "long_name": "T0068 - Respond to Breaking News Event or Active Crisis",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0072",
          "name": "Segment Audiences",
          "tactic_id": "TA13",
          "summary": "Create audience segmentations by features of interest to the influence campaign, including political affiliation, geographic location, income, demographics, and psychographics.",
          "changes_from_v0_1": "no change",
          "long_name": "T0072 - Segment Audiences",
          "sub_techniques": [
            {
              "disarm_id": "T0072.001",
              "name": "Geographic Segmentation",
              "tactic_id": "TA13",
              "summary": "An influence operation may target populations in a specific geographic location, such as a region, state, or city. An influence operation may use geographic segmentation to Create Localised Content (see: Establish Legitimacy).",
              "changes_from_v0_1": "no change",
              "long_name": "Geographic Segmentation -"
            },
            {
              "disarm_id": "T0072.002",
              "name": "Demographic Segmentation",
              "tactic_id": "TA13",
              "summary": "An influence operation may target populations based on demographic segmentation, including age, gender, and income. Demographic segmentation may be useful for influence operations aiming to change state policies that affect a specific population sector. For example, an influence operation attempting to influence Medicare funding in the United States would likely target U.S. voters over 65 years of age.",
              "changes_from_v0_1": "no change",
              "long_name": "Demographic Segmentation -"
            },
            {
              "disarm_id": "T0072.003",
              "name": "Economic Segmentation",
              "tactic_id": "TA13",
              "summary": "An influence operation may target populations based on their income bracket, wealth, or other financial or economic division.",
              "changes_from_v0_1": "no change",
              "long_name": "Economic Segmentation -"
            },
            {
              "disarm_id": "T0072.004",
              "name": "Psychographic Segmentation",
              "tactic_id": "TA13",
              "summary": "An influence operation may target populations based on psychographic segmentation, which uses audience values and decision-making processes. An operation may individually gather psychographic data with its own surveys or collection tools or externally purchase data from social media companies or online surveys, such as personality quizzes.",
              "changes_from_v0_1": "no change",
              "long_name": "Psychographic Segmentation -"
            },
            {
              "disarm_id": "T0072.005",
              "name": "Political Segmentation",
              "tactic_id": "TA13",
              "summary": "An influence operation may target populations based on their political affiliations, especially when aiming to manipulate voting or change policy.",
              "changes_from_v0_1": "no change",
              "long_name": "Political Segmentation -"
            }
          ]
        },
        {
          "disarm_id": "T0073",
          "name": "Determine Target Audiences",
          "tactic_id": "TA01",
          "summary": "Determining the target audiences (segments of the population) who will receive campaign narratives and artefacts intended to achieve the strategic ends.",
          "changes_from_v0_1": "new",
          "long_name": "T0073 - Determine Target Audiences",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0074",
          "name": "Determine Strategic Ends",
          "tactic_id": "TA01",
          "summary": "These are the long-term end-states the campaign aims to bring about. They typically involve an advantageous position vis-a-vis competitors in terms of power or influence.  The strategic goal may be to improve or simply to hold one’s position. Competition occurs in the public sphere in the domains of war, diplomacy, politics, economics, and ideology, and can play out between armed groups, nation-states, political parties, corporations, interest groups, or individuals. ",
          "changes_from_v0_1": "Amended in V1.3",
          "long_name": "T0074 - Determine Strategic Ends",
          "sub_techniques": [
            {
              "disarm_id": "T0074.001",
              "name": "Geopolitical Advantage",
              "tactic_id": "TA01",
              "summary": "Favourable position on the international stage in terms of great power politics or regional rivalry. Geopolitics plays out in the realms of foreign policy, national security, diplomacy, and intelligence. It involves nation-state governments, heads of state, foreign ministers, intergovernmental organisations, and regional security alliances.",
              "changes_from_v0_1": "New in V1.3",
              "long_name": "Geopolitical Advantage -"
            },
            {
              "disarm_id": "T0074.002",
              "name": "Domestic Political Advantage",
              "tactic_id": "TA01",
              "summary": "Favourable position vis-à-vis national or sub-national political opponents such as political parties, interest groups, politicians, candidates.  ",
              "changes_from_v0_1": "New in V1.3",
              "long_name": "Domestic Political Advantage -"
            },
            {
              "disarm_id": "T0074.003",
              "name": "Economic Advantage",
              "tactic_id": "TA01",
              "summary": "Favourable position domestically or internationally in the realms of commerce, trade, finance, industry. Economics involves nation-states, corporations, banks, trade blocs, industry associations, cartels.  ",
              "changes_from_v0_1": "New in V1.3",
              "long_name": "Economic Advantage -"
            },
            {
              "disarm_id": "T0074.004",
              "name": "Ideological Advantage",
              "tactic_id": "TA01",
              "summary": "Favourable position domestically or internationally in the market for ideas, beliefs, and world views. Competition plays out among faith systems, political systems, and value systems. It can involve sub-national, national or supra-national movements.    ",
              "changes_from_v0_1": "New in V1.3",
              "long_name": "Ideological Advantage -"
            }
          ]
        },
        {
          "disarm_id": "T0075",
          "name": "Dismiss",
          "tactic_id": "TA02",
          "summary": "Push back against criticism by dismissing your critics. This might be arguing that the critics use a different standard for you than with other actors or themselves; or arguing that their criticism is biassed.",
          "changes_from_v0_1": "Split from T0001",
          "long_name": "T0075 - Dismiss",
          "sub_techniques": [
            {
              "disarm_id": "T0075.001",
              "name": "Discredit Credible Sources",
              "tactic_id": "TA02",
              "summary": "Plan to delegitimize the media landscape and degrade public trust in reporting, by discrediting credible sources. This makes it easier to promote influence operation content.",
              "changes_from_v0_1": "no change",
              "long_name": "Discredit Credible Sources -"
            }
          ]
        },
        {
          "disarm_id": "T0076",
          "name": "Distort",
          "tactic_id": "TA02",
          "summary": "Twist the narrative. Take information, or artefacts like images, and change the framing around them.",
          "changes_from_v0_1": "Split from T0001",
          "long_name": "T0076 - Distort",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0077",
          "name": "Distract",
          "tactic_id": "TA02",
          "summary": "Shift attention to a different narrative or actor, for instance by accusing critics of the same activity that they’ve accused you of (e.g. police brutality).",
          "changes_from_v0_1": "Split from T0001",
          "long_name": "T0077 - Distract",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0078",
          "name": "Dismay",
          "tactic_id": "TA02",
          "summary": "Threaten the critic or narrator of events. For instance, threaten journalists or news outlets reporting on a story.",
          "changes_from_v0_1": "Split from T0001",
          "long_name": "T0078 - Dismay",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0079",
          "name": "Divide",
          "tactic_id": "TA02",
          "summary": "Create conflict between subgroups, to widen divisions in a community",
          "changes_from_v0_1": "Split from T0001",
          "long_name": "T0079 - Divide",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0080",
          "name": "Map Target Audience Information Environment",
          "tactic_id": "TA13",
          "summary": "Mapping the target audience information environment analyses the information space itself, including social media analytics, web traffic, and media surveys. Mapping the information environment may help the influence operation determine the most realistic and popular information channels to reach its target audience. Mapping the target audience information environment aids influence operations in determining the most vulnerable areas of the information space to target with messaging.",
          "changes_from_v0_1": "new",
          "long_name": "T0080 - Map Target Audience Information Environment",
          "sub_techniques": [
            {
              "disarm_id": "T0080.001",
              "name": "Monitor Social Media Analytics",
              "tactic_id": "TA13",
              "summary": "An influence operation may use social media analytics to determine which factors will increase the operation content’s exposure to its target audience on social media platforms, including views, interactions, and sentiment relating to topics and content types. The social media platform itself or a third-party tool may collect the metrics.",
              "changes_from_v0_1": "no change",
              "long_name": "Monitor Social Media Analytics -"
            },
            {
              "disarm_id": "T0080.002",
              "name": "Evaluate Media Surveys",
              "tactic_id": "TA13",
              "summary": "An influence operation may evaluate its own or third-party media surveys to determine what type of content appeals to its target audience. Media surveys may provide insight into an audience’s political views, social class, general interests, or other indicators used to tailor operation messaging to its target audience.",
              "changes_from_v0_1": "no change",
              "long_name": "Evaluate Media Surveys -"
            },
            {
              "disarm_id": "T0080.003",
              "name": "Identify Trending Topics/Hashtags",
              "tactic_id": "TA13",
              "summary": "An influence operation may identify trending hashtags on social media platforms for later use in boosting operation content. A hashtag40 refers to a word or phrase preceded by the hash symbol (#) on social media used to identify messages and posts relating to a specific topic. All public posts that use the same hashtag are aggregated onto a centralised page dedicated to the word or phrase and sorted either chronologically or by popularity.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Trending Topics/Hashtags -"
            },
            {
              "disarm_id": "T0080.004",
              "name": "Conduct Web Traffic Analysis",
              "tactic_id": "TA13",
              "summary": "An influence operation may conduct web traffic analysis to determine which search engines, keywords, websites, and advertisements gain the most traction with its target audience.",
              "changes_from_v0_1": "no change",
              "long_name": "Conduct Web Traffic Analysis -"
            },
            {
              "disarm_id": "T0080.005",
              "name": "Assess Degree/Type of Media Access",
              "tactic_id": "TA13",
              "summary": "An influence operation may survey a target audience’s Internet availability and degree of media freedom to determine which target audience members will have access to operation content and on which platforms. An operation may face more difficulty targeting an information environment with heavy restrictions and media control than an environment with independent media, freedom of speech and of the press, and individual liberties.",
              "changes_from_v0_1": "no change",
              "long_name": "Assess Degree/Type of Media Access -"
            }
          ]
        },
        {
          "disarm_id": "T0081",
          "name": "Identify Social and Technical Vulnerabilities",
          "tactic_id": "TA13",
          "summary": "Identifying social and technical vulnerabilities determines weaknesses within the target audience information environment for later exploitation. Vulnerabilities include decisive political issues, weak cybersecurity infrastructure, search engine data voids, and other technical and non technical weaknesses in the target information environment. Identifying social and technical vulnerabilities facilitates the later exploitation of the identified weaknesses to advance operation objectives.",
          "changes_from_v0_1": "new",
          "long_name": "T0081 - Identify Social and Technical Vulnerabilities",
          "sub_techniques": [
            {
              "disarm_id": "T0081.001",
              "name": "Find Echo Chambers",
              "tactic_id": "TA13",
              "summary": "Find or plan to create areas (social media groups, search term groups, hashtag groups etc) where individuals only engage with people they agree with.",
              "changes_from_v0_1": "no change",
              "long_name": "Find Echo Chambers -"
            },
            {
              "disarm_id": "T0081.002",
              "name": "Identify Data Voids",
              "tactic_id": "TA13",
              "summary": "A data void refers to a word or phrase that results in little, manipulative, or low-quality search engine data. Data voids are hard to detect and relatively harmless until exploited by an entity aiming to quickly proliferate false or misleading information during a phenomenon that causes a high number of individuals to query the term or phrase. In the Plan phase, an influence operation may identify data voids for later exploitation in the operation. A 2019 report by Michael Golebiewski identifies five types of data voids. (1) “Breaking news” data voids occur when a keyword gains popularity during a short period of time, allowing an influence operation to publish false content before legitimate news outlets have an opportunity to publish relevant information. (2) An influence operation may create a “strategic new terms” data void by creating their own terms and publishing information online before promoting their keyword to the target audience. (3) An influence operation may publish content on “outdated terms” that have decreased in popularity, capitalising on most search engines’ preferences for recency. (4) “Fragmented concepts” data voids separate connections between similar ideas, isolating segment queries to distinct search engine results. (5) An influence operation may use “problematic queries” that previously resulted in disturbing or inappropriate content to promote messaging until mainstream media recontextualizes the term.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Data Voids -"
            },
            {
              "disarm_id": "T0081.003",
              "name": "Identify Existing Prejudices",
              "tactic_id": "TA13",
              "summary": "An influence operation may exploit existing racial, religious, demographic, or social prejudices to further polarise its target audience from the rest of the public.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Existing Prejudices -"
            },
            {
              "disarm_id": "T0081.004",
              "name": "Identify Existing Fissures",
              "tactic_id": "TA13",
              "summary": "An influence operation may identify existing fissures to pit target populations against one another or facilitate a “divide-and-conquer\" approach to tailor operation narratives along the divides.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Existing Fissures -"
            },
            {
              "disarm_id": "T0081.005",
              "name": "Identify Existing Conspiracy Narratives/Suspicions",
              "tactic_id": "TA13",
              "summary": "An influence operation may assess preexisting conspiracy theories or suspicions in a population to identify existing narratives that support operational objectives.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Existing Conspiracy Narratives/Suspicions -"
            },
            {
              "disarm_id": "T0081.006",
              "name": "Identify Wedge Issues",
              "tactic_id": "TA13",
              "summary": "A wedge issue is a divisive political issue, usually concerning a social phenomenon, that divides individuals along a defined line. An influence operation may exploit wedge issues by intentionally polarising the public along the wedge issue line and encouraging opposition between factions.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Wedge Issues -"
            },
            {
              "disarm_id": "T0081.007",
              "name": "Identify Target Audience Adversaries",
              "tactic_id": "TA13",
              "summary": "An influence operation may identify or create a real or imaginary adversary to centre operation narratives against. A real adversary may include certain politicians or political parties while imaginary adversaries may include falsified “deep state”62 actors that, according to conspiracies, run the state behind public view.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Target Audience Adversaries -"
            },
            {
              "disarm_id": "T0081.008",
              "name": "Identify Media System Vulnerabilities",
              "tactic_id": "TA13",
              "summary": "An influence operation may exploit existing weaknesses in a target’s media system. These weaknesses may include existing biases among media agencies, vulnerability to false news agencies on social media, or existing distrust of traditional media sources. An existing distrust among the public in the media system’s credibility holds high potential for exploitation by an influence operation when establishing alternative news agencies to spread operation content.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Media System Vulnerabilities -"
            }
          ]
        },
        {
          "disarm_id": "T0082",
          "name": "Develop New Narratives",
          "tactic_id": "TA14",
          "summary": "Actors may develop new narratives to further strategic or tactical goals, especially when existing narratives adequately align with the campaign goals. New narratives provide more control in terms of crafting the message to achieve specific goals. However, new narratives may require more effort to disseminate than adapting or adopting existing narratives.",
          "changes_from_v0_1": "new",
          "long_name": "T0082 - Develop New Narratives",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0083",
          "name": "Integrate Target Audience Vulnerabilities into Narrative",
          "tactic_id": "TA14",
          "summary": "An influence operation may seek to exploit the preexisting weaknesses, fears, and enemies of the target audience for integration into the operation’s narratives and overall strategy. Integrating existing vulnerabilities into the operational approach conserves resources by exploiting already weak areas of the target information environment instead of forcing the operation to create new vulnerabilities in the environment.",
          "changes_from_v0_1": "new",
          "long_name": "T0083 - Integrate Target Audience Vulnerabilities into Narrative",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0084",
          "name": "Reuse Existing Content",
          "tactic_id": "TA06",
          "summary": "When an operation recycles content from its own previous operations or plagiarises from external operations. An operation may launder information to conserve resources that would have otherwise been utilised to develop new content.",
          "changes_from_v0_1": "new",
          "long_name": "T0084 - Reuse Existing Content",
          "sub_techniques": [
            {
              "disarm_id": "T0084.001",
              "name": "Use Copypasta",
              "tactic_id": "TA06",
              "summary": "Copypasta refers to a piece of text that has been copied and pasted multiple times across various online platforms. A copypasta’s final form may differ from its original source text as users add, delete, or otherwise edit the content as they repost the text.",
              "changes_from_v0_1": "no change",
              "long_name": "Use Copypasta -"
            },
            {
              "disarm_id": "T0084.002",
              "name": "Plagiarise Content",
              "tactic_id": "TA06",
              "summary": "An influence operation may take content from other sources without proper attribution. This content may be either misinformation content shared by others without malicious intent but now leveraged by the campaign as disinformation or disinformation content from other sources.",
              "changes_from_v0_1": "no change",
              "long_name": "Plagiarise Content -"
            },
            {
              "disarm_id": "T0084.003",
              "name": "Deceptively Labelled or Translated",
              "tactic_id": "TA06",
              "summary": "An influence operation may take authentic content from other sources and add deceptive labels or deceptively translate the content into other langauges.",
              "changes_from_v0_1": "no change",
              "long_name": "Deceptively Labelled or Translated -"
            },
            {
              "disarm_id": "T0084.004",
              "name": "Appropriate Content",
              "tactic_id": "TA06",
              "summary": "An influence operation may take content from other sources with proper attribution. This content may be either misinformation content shared by others without malicious intent but now leveraged by the campaign as disinformation or disinformation content from other sources. Examples include the appropriation of content from one inauthentic news site to another inauthentic news site or network in ways that align with the originators licencing or terms of service.",
              "changes_from_v0_1": "no change",
              "long_name": "Appropriate Content -"
            }
          ]
        },
        {
          "disarm_id": "T0085",
          "name": "Develop Text-Based Content",
          "tactic_id": "TA06",
          "summary": "Creating and editing false or misleading text-based artefacts, often aligned with one or more specific narratives, for use in a disinformation campaign.",
          "changes_from_v0_1": "new",
          "long_name": "T0085 - Develop Text-Based Content",
          "sub_techniques": [
            {
              "disarm_id": "T0085.001",
              "name": "Develop AI-Generated Text",
              "tactic_id": "TA06",
              "summary": "AI-generated texts refers to synthetic text composed by computers using text-generating AI technology. Autonomous generation refers to content created by a bot without human input, also known as bot-created content generation. Autonomous generation represents the next step in automation after language generation and may lead to automated journalism. An influence operation may use read fakes or autonomous generation to quickly develop and distribute content to the target audience.",
              "changes_from_v0_1": "no change",
              "long_name": "Develop AI-Generated Text -"
            },
            {
              "disarm_id": "T0085.003",
              "name": "Develop Inauthentic News Articles",
              "tactic_id": "TA06",
              "summary": "An influence operation may develop false or misleading news articles aligned to their campaign goals or narratives.",
              "changes_from_v0_1": "no change",
              "long_name": "Develop Inauthentic News Articles -"
            },
            {
              "disarm_id": "T0085.004",
              "name": "Develop Document",
              "tactic_id": "TA06",
              "summary": "Produce text in the form of a document.",
              "changes_from_v0_1": "Introduced V1.4",
              "long_name": "Develop Document"
            },
            {
              "disarm_id": "T0085.005",
              "name": "Develop Book",
              "tactic_id": "TA06",
              "summary": "Produce text content in the form of a book. \n \nThis technique covers both e-books and physical books, however, the former is more easily deployed by threat actors given the lower cost to develop.",
              "changes_from_v0_1": "Introduced V1.4",
              "long_name": "Develop Book"
            },
            {
              "disarm_id": "T0085.006",
              "name": "Develop Opinion Article",
              "tactic_id": "TA06",
              "summary": "Opinion articles (aka “Op-Eds” or “Editorials”) are articles or regular columns flagged as “opinion” posted to news sources, and can be contributed by people outside the organisation. \n \nFlagging articles as opinions allow news organisations to distinguish them from the typical expectations of objective news reporting while distancing the presented opinion from the organisation or its employees.\n \nThe use of this technique is not by itself an indication of malicious or inauthentic content; Op-eds are a common format in media. However, threat actors exploit op-eds to, for example, submit opinion articles to local media to promote their narratives. \n \nExamples from the perspective of a news site involve publishing op-eds from perceived prestigious voices to give legitimacy to an inauthentic publication, or supporting causes by hosting op-eds from actors aligned with the organisation’s goals.",
              "changes_from_v0_1": "Introduced V1.4",
              "long_name": "Develop Opinion Article"
            },
            {
              "disarm_id": "T0085.007",
              "name": "Create Fake Research",
              "tactic_id": "TA06",
              "summary": "Create fake academic research. Example: fake social science research is often aimed at hot-button social issues such as gender, race and sexuality. Fake science research can target Climate Science debate or pseudoscience like anti-vaxx.\n \nThis Technique previously used the ID T0019.001.",
              "changes_from_v0_1": "Updated V1.4",
              "long_name": "Create Fake Research"
            },
            {
              "disarm_id": "T0085.008",
              "name": "Machine Translated Text",
              "tactic_id": "TA06",
              "summary": "Text which has been translated into another language using machine translation tools, such as AI.",
              "changes_from_v0_1": "Introduced V1.5",
              "long_name": "Machine Translated Text"
            }
          ]
        },
        {
          "disarm_id": "T0086",
          "name": "Develop Image-Based Content",
          "tactic_id": "TA06",
          "summary": "Creating and editing false or misleading visual artefacts, often aligned with one or more specific narratives, for use in a disinformation campaign. This may include photographing staged real-life situations, repurposing existing digital images, or using image creation and editing technologies.",
          "changes_from_v0_1": "new",
          "long_name": "T0086 - Develop Image-Based Content",
          "sub_techniques": [
            {
              "disarm_id": "T0086.001",
              "name": "Develop Memes",
              "tactic_id": "TA06",
              "summary": "Memes are one of the most important single artefact types in all of computational propaganda. Memes in this framework denotes the narrow image-based definition. But that naming is no accident, as these items have most of the important properties of Dawkins' original conception as a self-replicating unit of culture. Memes pull together reference and commentary; image and narrative; emotion and message. Memes are a powerful tool and the heart of modern influence campaigns.",
              "changes_from_v0_1": "no change",
              "long_name": "Develop Memes -"
            },
            {
              "disarm_id": "T0086.002",
              "name": "Develop AI-Generated Images (Deepfakes)",
              "tactic_id": "TA06",
              "summary": "Deepfakes refer to AI-generated falsified photos, videos, or soundbites. An influence operation may use deepfakes to depict an inauthentic situation by synthetically recreating an individual’s face, body, voice, and physical gestures.",
              "changes_from_v0_1": "no change",
              "long_name": "Develop AI-Generated Images (Deepfakes) -"
            },
            {
              "disarm_id": "T0086.003",
              "name": "Deceptively Edit Images (Cheap Fakes)",
              "tactic_id": "TA06",
              "summary": "Cheap fakes utilise less sophisticated measures of altering an image, video, or audio for example, slowing, speeding, or cutting footage to create a false context surrounding an image or event.",
              "changes_from_v0_1": "no change",
              "long_name": "Deceptively Edit Images (Cheap Fakes) -"
            },
            {
              "disarm_id": "T0086.004",
              "name": "Aggregate Information into Evidence Collages",
              "tactic_id": "TA06",
              "summary": "Image files that aggregate positive evidence (Joan Donovan)",
              "changes_from_v0_1": "no change",
              "long_name": "Aggregate Information into Evidence Collages -"
            }
          ]
        },
        {
          "disarm_id": "T0087",
          "name": "Develop Video-Based Content",
          "tactic_id": "TA06",
          "summary": "Creating and editing false or misleading video artefacts, often aligned with one or more specific narratives, for use in a disinformation campaign. This may include staging videos of purportedly real situations, repurposing existing video artefacts, or using AI-generated video creation and editing technologies (including deepfakes).",
          "changes_from_v0_1": "new",
          "long_name": "T0087 - Develop Video-Based Content",
          "sub_techniques": [
            {
              "disarm_id": "T0087.001",
              "name": "Develop AI-Generated Videos (Deepfakes)",
              "tactic_id": "TA06",
              "summary": "Deepfakes refer to AI-generated falsified photos, videos, or soundbites. An influence operation may use deepfakes to depict an inauthentic situation by synthetically recreating an individual’s face, body, voice, and physical gestures.",
              "changes_from_v0_1": "no change",
              "long_name": "Develop AI-Generated Videos (Deepfakes) -"
            },
            {
              "disarm_id": "T0087.002",
              "name": "Deceptively Edit Video (Cheap Fakes)",
              "tactic_id": "TA06",
              "summary": "Cheap fakes utilise less sophisticated measures of altering an image, video, or audio for example, slowing, speeding, or cutting footage to create a false context surrounding an image or event.",
              "changes_from_v0_1": "no change",
              "long_name": "Deceptively Edit Video (Cheap Fakes) -"
            }
          ]
        },
        {
          "disarm_id": "T0088",
          "name": "Develop Audio-Based Content",
          "tactic_id": "TA06",
          "summary": "Creating and editing false or misleading audio artefacts, often aligned with one or more specific narratives, for use in a disinformation campaign. This may include creating completely new audio content, repurposing existing audio artefacts (including cheap fakes), or using AI-generated audio creation and editing technologies (including deepfakes).",
          "changes_from_v0_1": "new",
          "long_name": "T0088 - Develop Audio-Based Content",
          "sub_techniques": [
            {
              "disarm_id": "T0088.001",
              "name": "Develop AI-Generated Audio (Deepfakes)",
              "tactic_id": "TA06",
              "summary": "Deepfakes refer to AI-generated falsified photos, videos, or soundbites. An influence operation may use deepfakes to depict an inauthentic situation by synthetically recreating an individual’s face, body, voice, and physical gestures.",
              "changes_from_v0_1": "no change",
              "long_name": "Develop AI-Generated Audio (Deepfakes) -"
            },
            {
              "disarm_id": "T0088.002",
              "name": "Deceptively Edit Audio (Cheap Fakes)",
              "tactic_id": "TA06",
              "summary": "Cheap fakes utilise less sophisticated measures of altering an image, video, or audio for example, slowing, speeding, or cutting footage to create a false context surrounding an image or event.",
              "changes_from_v0_1": "no change",
              "long_name": "Deceptively Edit Audio (Cheap Fakes) -"
            }
          ]
        },
        {
          "disarm_id": "T0089",
          "name": "Obtain Private Documents",
          "tactic_id": "TA06",
          "summary": "Procuring documents that are not publicly available, by whatever means -- whether legal or illegal, highly-resourced or less so. These documents can include authentic non-public documents, authentic non-public documents have been altered, or inauthentic documents intended to appear as if they are authentic non-public documents. All of these types of documents can be \"leaked\" during later stages in the operation.",
          "changes_from_v0_1": "new",
          "long_name": "T0089 - Obtain Private Documents",
          "sub_techniques": [
            {
              "disarm_id": "T0089.001",
              "name": "Obtain Authentic Documents",
              "tactic_id": "TA06",
              "summary": "Procure authentic documents that are not publicly available, by whatever means -- whether legal or illegal, highly-resourced or less so. These documents can be \"leaked\" during later stages in the operation.",
              "changes_from_v0_1": "no change",
              "long_name": "Obtain Authentic Documents -"
            },
            {
              "disarm_id": "T0089.003",
              "name": "Alter Authentic Documents",
              "tactic_id": "TA06",
              "summary": "Alter authentic documents (public or non-public) to achieve campaign goals. The altered documents are intended to appear as if they are authentic and can be \"leaked\" during later stages in the operation.",
              "changes_from_v0_1": "no change",
              "long_name": "Alter Authentic Documents -"
            }
          ]
        },
        {
          "disarm_id": "T0091",
          "name": "Recruit Malign Actors",
          "tactic_id": "TA15",
          "summary": "Operators recruit bad actors paying recruiting, or exerting control over individuals includes trolls, partisans, and contractors.",
          "changes_from_v0_1": "new",
          "long_name": "T0091 - Recruit Malign Actors",
          "sub_techniques": [
            {
              "disarm_id": "T0091.001",
              "name": "Recruit Contractors",
              "tactic_id": "TA15",
              "summary": "Operators recruit paid contractor to support the campaign.",
              "changes_from_v0_1": "no change",
              "long_name": "Recruit Contractors -"
            },
            {
              "disarm_id": "T0091.002",
              "name": "Recruit Partisans",
              "tactic_id": "TA15",
              "summary": "Operators recruit partisans (ideologically-aligned individuals) to support the campaign.",
              "changes_from_v0_1": "no change",
              "long_name": "Recruit Partisans -"
            },
            {
              "disarm_id": "T0091.003",
              "name": "Enlist Troll Accounts",
              "tactic_id": "TA15",
              "summary": "An influence operation may hire trolls, or human operators of fake accounts that aim to provoke others by posting and amplifying content about controversial issues. Trolls can serve to discredit an influence operation’s opposition or bring attention to the operation’s cause through debate. Classic trolls refer to regular people who troll for personal reasons, such as attention-seeking or boredom. Classic trolls may advance operation narratives by coincidence but are not directly affiliated with any larger operation. Conversely, hybrid trolls act on behalf of another institution, such as a state or financial organisation, and post content with a specific ideological goal. Hybrid trolls may be highly advanced and institutionalised or less organised and work for a single individual.",
              "changes_from_v0_1": "no change",
              "long_name": "Enlist Troll Accounts -"
            }
          ]
        },
        {
          "disarm_id": "T0092",
          "name": "Build Network",
          "tactic_id": "TA15",
          "summary": "Operators build their own network, creating links between accounts -- whether authentic or inauthentic -- in order amplify and promote narratives and artefacts, and encourage further growth of ther network, as well as the ongoing sharing and engagement with operational content.",
          "changes_from_v0_1": "new",
          "long_name": "T0092 - Build Network",
          "sub_techniques": [
            {
              "disarm_id": "T0092.001",
              "name": "Create Organisations",
              "tactic_id": "TA15",
              "summary": "Influence operations may establish organisations with legitimate or falsified hierarchies, staff, and content to structure operation assets, provide a sense of legitimacy to the operation, or provide institutional backing to operation activities.",
              "changes_from_v0_1": "no change",
              "long_name": "Create Organisations -"
            },
            {
              "disarm_id": "T0092.002",
              "name": "Use Follow Trains",
              "tactic_id": "TA15",
              "summary": "A follow train is a group of people who follow each other on a social media platform, often as a way for an individual or campaign to grow its social media following. Follow trains may be a violation of platform Terms of Service. They are also known as follow-for-follow groups.",
              "changes_from_v0_1": "no change",
              "long_name": "Use Follow Trains -"
            },
            {
              "disarm_id": "T0092.003",
              "name": "Create Community or Sub-Group",
              "tactic_id": "TA15",
              "summary": "When there is not an existing community or sub-group that meets a campaign's goals, an influence operation may seek to create a community or sub-group.",
              "changes_from_v0_1": "no change",
              "long_name": "Create Community or Sub-Group -"
            }
          ]
        },
        {
          "disarm_id": "T0093",
          "name": "Acquire/Recruit Network",
          "tactic_id": "TA15",
          "summary": "Operators acquire an existing network by paying, recruiting, or exerting control over the leaders of the existing network.",
          "changes_from_v0_1": "new",
          "long_name": "T0093 - Acquire/Recruit Network",
          "sub_techniques": [
            {
              "disarm_id": "T0093.001",
              "name": "Fund Proxies",
              "tactic_id": "TA15",
              "summary": "An influence operation may fund proxies, or external entities that work for the operation. An operation may recruit/train users with existing sympathies towards the operation’s narratives and/or goals as proxies. Funding proxies serves various purposes including: - Diversifying operation locations to complicate attribution - Reducing the workload for direct operation assets",
              "changes_from_v0_1": "no change",
              "long_name": "Fund Proxies -"
            },
            {
              "disarm_id": "T0093.002",
              "name": "Acquire Botnets",
              "tactic_id": "TA15",
              "summary": "A botnet is a group of bots that can function in coordination with each other.",
              "changes_from_v0_1": "no change",
              "long_name": "Acquire Botnets -"
            }
          ]
        },
        {
          "disarm_id": "T0094",
          "name": "Infiltrate Existing Networks",
          "tactic_id": "TA15",
          "summary": "Operators deceptively insert social assets into existing networks as group members in order to influence the members of the network and the wider information environment that the network impacts.",
          "changes_from_v0_1": "new",
          "long_name": "T0094 - Infiltrate Existing Networks",
          "sub_techniques": [
            {
              "disarm_id": "T0094.001",
              "name": "Identify Susceptible Targets in Networks",
              "tactic_id": "TA15",
              "summary": "When seeking to infiltrate an existing network, an influence operation may identify individuals and groups that might be susceptible to being co-opted or influenced.",
              "changes_from_v0_1": "no change",
              "long_name": "Identify Susceptible Targets in Networks -"
            },
            {
              "disarm_id": "T0094.002",
              "name": "Utilise Butterfly Attacks",
              "tactic_id": "TA15",
              "summary": "Butterfly attacks occur when operators pretend to be members of a certain social group, usually a group that struggles for representation. An influence operation may mimic a group to insert controversial statements into the discourse, encourage the spread of operation content, or promote harassment among group members. Unlike astroturfing, butterfly attacks aim to infiltrate and discredit existing grassroots movements, organisations, and media campaigns.",
              "changes_from_v0_1": "no change",
              "long_name": "Utilise Butterfly Attacks -"
            }
          ]
        },
        {
          "disarm_id": "T0095",
          "name": "Develop Owned Media Assets",
          "tactic_id": "TA15",
          "summary": "An owned media asset refers to an agency or organisation through which an influence operation may create, develop, and host content and narratives. Owned media assets include websites, blogs, social media pages, forums, and other platforms that facilitate the creation and organisation of content.",
          "changes_from_v0_1": "new",
          "long_name": "T0095 - Develop Owned Media Assets",
          "sub_techniques": []
        },
        {
          "disarm_id": "T0096",
          "name": "Leverage Content Farms",
          "tactic_id": "TA15",
          "summary": "Using the services of large-scale content providers for creating and amplifying campaign artefacts at scale.",
          "changes_from_v0_1": "new",
          "long_name": "T0096 - Leverage Content Farms",
          "sub_techniques": [
            {
              "disarm_id": "T0096.001",
              "name": "Create Content Farms",
              "tactic_id": "TA15",
              "summary": "An influence operation may create an organisation for creating and amplifying campaign artefacts at scale.",
              "changes_from_v0_1": "no change",
              "long_name": "Create Content Farms -"
            },
            {
              "disarm_id": "T0096.002",
              "name": "Outsource Content Creation to External Organisations",
              "tactic_id": "TA15",
              "summary": "An influence operation may outsource content creation to external companies to avoid attribution, increase the rate of content creation, or improve content quality, i.e., by employing an organisation that can create content in the target audience’s native language. Employed organisations may include marketing companies for tailored advertisements or external content farms for high volumes of targeted media.",
              "changes_from_v0_1": "no change",
              "long_name": "Outsource Content Creation to External Organisations -"
            }
          ]
        },
        {
          "disarm_id": "T0097",
          "name": "Present Persona",
          "tactic_id": "TA16",
          "summary": "This Technique contains different types of personas commonly taken on by threat actors during influence operations.\n\nAnalysts should use T0097’s sub-techniques to document the type of persona which an account is presenting. For example, an account which describes itself as being a journalist can be tagged with T0097.102: Journalist Persona.\n\nPersonas presented by individuals include:\n\nT0097.100: Individual Persona\nT0097.101: Local Persona\nT0097.102: Journalist Persona\nT0097.103: Activist Persona\nT0097.104: Hacktivist Persona\nT0097.105: Military Personnel Persona\nT0097.106: Recruiter Persona\nT0097.107: Researcher Persona\nT0097.108: Expert Persona\nT0097.109: Romantic Suitor Persona\nT0097.110: Party Official Persona\nT0097.111: Government Official Persona\nT0097.112: Government Employee Persona\n\nThis Technique also houses institutional personas commonly taken on by threat actors:\n\nT0097.200: Institutional Persona\nT0097.201: Local Institution Persona\nT0097.202: News Outlet Persona\nT0097.203: Fact Checking Organisation Persona\nT0097.204: Think Tank Persona\nT0097.205: Business Persona\nT0097.206: Government Institution Persona\nT0097.207: NGO Persona\nT0097.208: Social Cause Persona\n\nBy using a persona, a threat actor is adding the perceived legitimacy of the persona to their narratives and activities.",
          "changes_from_v0_1": "new",
          "long_name": "T0097 - Present Persona",
          "sub_techniques": [
            {
              "disarm_id": "T0097.100",
              "name": "Individual Persona",
              "tactic_id": "TA16",
              "summary": "This sub-technique can be used to indicate that an entity is presenting itself as an individual. If the person is presenting themselves as having one of the personas listed below then these sub-techniques should be used instead, as they indicate both the type of persona they presented and that the entity presented itself as an individual:\n\nT0097.101: Local Persona\nT0097.102: Journalist Persona\nT0097.103: Activist Persona\nT0097.104: Hacktivist Persona\nT0097.105: Military Personnel Persona\nT0097.106: Recruiter Persona\nT0097.107: Researcher Persona\nT0097.108: Expert Persona\nT0097.109: Romantic Suitor Persona\nT0097.110: Party Official Persona\nT0097.111: Government Official Persona\nT0097.112: Government Employee Persona",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.101",
              "name": "Local Persona",
              "tactic_id": "TA16",
              "summary": "A person with a local persona presents themselves as living in a particular geography or having local knowledge relevant to a narrative.\n\nWhile presenting as a local is not an indication of inauthentic behaviour,  an influence operation may have its narratives amplified by people presenting as local to a target area. Threat actors can fabricate locals (T0143.002: Fabricated Persona, T0097.101: Local Persona) to add credibility to their narratives, or to misrepresent the real opinions of locals in the area.\n\nPeople who are legitimate locals (T0143.001: Authentic Persona, T0097.101: Local Persona) can use their persona for malicious purposes, or be exploited by threat actors. For example, someone could take money for using their position as a local to provide legitimacy to a false narrative or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.102",
              "name": "Journalist Persona",
              "tactic_id": "TA16",
              "summary": "A person with a journalist persona presents themselves as a reporter or journalist delivering news, conducting interviews, investigations etc.\n\nWhile presenting as a journalist is not an indication of inauthentic behaviour, an influence operation may have its narratives amplified by people presenting as journalists. Threat actors can fabricate journalists to give the appearance of legitimacy, justifying the actor’s requests for interviews, etc (T0143.002: Fabricated Persona, T0097.102: Journalist Persona).\n\nPeople who have legitimately developed a persona as a journalist (T0143.001: Authentic Persona, T0097.102: Journalist Persona) can use it for malicious purposes, or be exploited by threat actors. For example, someone could take money for using their position as a trusted journalist to provide legitimacy to a false narrative or be tricked into doing so without the journalist’s knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.103",
              "name": "Activist Persona",
              "tactic_id": "TA16",
              "summary": "A person with an activist persona presents themselves as an activist; an individual who campaigns for a political cause, organises related events, etc.\n\nWhile presenting as an activist is not an indication of inauthentic behaviour, an influence operation may have its narratives amplified by people presenting as activists. Threat actors can fabricate activists to give the appearance of popular support for an evolving grassroots movement (see T0143.002: Fabricated Persona, T0097.103: Activist Persona).\n\nPeople who are legitimate activists can use this persona for malicious purposes, or be exploited by threat actors. For example, someone could take money for using their position as an activist to provide visibility to a false narrative or be tricked into doing so without their knowledge (T0143.001: Authentic Persona, T0097.103: Activist Persona).",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.104",
              "name": "Hacktivist Persona",
              "tactic_id": "TA16",
              "summary": "A person with a hacktivist persona presents themselves as an activist who conducts offensive cyber operations or builds technical infrastructure for political purposes, rather than the financial motivations commonly attributed to hackers; hacktivists are hacker activists who use their technical knowledge to take political action.\n\nHacktivists can build technical infrastructure to support other activists, including secure communication channels and surveillance and censorship circumvention. They can also conduct DDOS attacks and other offensive cyber operations, aiming to take down digital assets or gain access to proprietary information. An influence operation may use hacktivist personas to support their operational narratives and legitimise their operational activities.\n\nFabricated Hacktivists are sometimes referred to as “Faketivists”.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.105",
              "name": "Military Personnel Persona",
              "tactic_id": "TA16",
              "summary": "A person with a military personnel persona presents themselves as a serving member or veteran of a military organisation operating in an official capacity on behalf of a government.\n\nWhile presenting as military personnel is not an indication of inauthentic behaviour,  an influence operation may have its narratives amplified by people presenting as military personnel. Threat actors can fabricate military personnel (T0143.002: Fabricated Persona, T0097.105: Military Personnel Persona) to pose as experts on military topics, or to discredit geopolitical adversaries by pretending to be one of their military personnel and spreading discontent.\n\nPeople who have legitimately developed a military persona (T0143.001: Authentic Persona, T0097.105: Military Personnel Persona) can use it for malicious purposes, or be exploited by threat actors. For example, someone could take money for using their position as a member of the military to provide legitimacy to a false narrative or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.106",
              "name": "Recruiter Persona",
              "tactic_id": "TA16",
              "summary": "A person with a recruiter persona presents themselves as a potential employer or provider of freelance work.\n\nWhile presenting as a recruiter is not an indication of inauthentic behaviour, threat actors fabricate recruiters (T0143.002: Fabricated Persona, T0097.106: Recruiter Persona) to justify asking for personal information from their targets or to trick targets into working for the threat actors (without revealing who they are).",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.107",
              "name": "Researcher Persona",
              "tactic_id": "TA16",
              "summary": "A person with a researcher persona presents themselves as conducting research (e.g. for academic institutions, or think tanks), or having previously conducted research.\n\nWhile presenting as a researcher is not an indication of inauthentic behaviour,  an influence operation may have its narratives amplified by people presenting as researchers. Threat actors can fabricate researchers (T0143.002: Fabricated Persona, T0097.107: Researcher Persona) to add credibility to their narratives.\n\nPeople who are legitimate researchers (T0143.001: Authentic Persona, T0097.107: Researcher Persona) can use their persona for malicious purposes, or be exploited by threat actors. For example, someone could take money for using their position as a Researcher to provide legitimacy to a false narrative or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.108",
              "name": "Expert Persona",
              "tactic_id": "TA16",
              "summary": "A person with an expert persona presents themselves as having expertise or experience in a field. Commonly the persona’s expertise will be called upon to add credibility to a given narrative.\n\nWhile presenting as an expert is not an indication of inauthentic behaviour,  an influence operation may have its narratives amplified by people presenting as experts. Threat actors can fabricate experts (T0143.002: Fabricated Persona, T0097.107: Researcher Persona) to add credibility to their narratives.\n\nPeople who are legitimate experts (T0143.001: Authentic Persona, T0097.107: Researcher Persona) can make mistakes, use their persona for malicious purposes, or be exploited by threat actors. For example, someone could take money for using their position as an expert to provide legitimacy to a false narrative or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.109",
              "name": "Romantic Suitor Persona",
              "tactic_id": "TA16",
              "summary": "A person with a romantic suitor persona presents themselves as seeking a romantic or physical connection with another person.\n\nWhile presenting as seeking a romantic or physical connection is not an indication of inauthentic behaviour, threat actors can use dating apps, social media channels or dating websites to fabricate romantic suitors to lure targets they can blackmail, extract information from, deceive or trick into giving them money (T0143.002: Fabricated Persona, T0097.109: Romantic Suitor Persona).\n\nHoneypotting in espionage and Big Butchering in scamming are commonly associated with romantic suitor personas.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.110",
              "name": "Party Official Persona",
              "tactic_id": "TA16",
              "summary": "A person who presents as an official member of a political party, such as leaders of political parties, candidates standing to represent constituents, and campaign staff.\n\nPresenting as an official of a political party is not an indication of inauthentic behaviour, however threat actors may fabricate individuals who work in political parties to add credibility to their narratives (T0143.002: Fabricated Persona, T0097.110: Party Official Persona). They may also impersonate existing officials of political parties (T0143.003: Impersonated Persona, T0097.110: Party Official Persona).\n\nLegitimate members of political parties could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.110: Party Official Persona). For example, an electoral candidate could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.111",
              "name": "Government Official Persona",
              "tactic_id": "TA16",
              "summary": "A person who presents as an active or previous government official has the government official persona. These are officials serving in government, such as heads of government departments, leaders of countries, and members of government selected to represent constituents.\n\n Presenting as a government official is not an indication of inauthentic behaviour, however threat actors may fabricate individuals who work in government to add credibility to their narratives (T0143.002: Fabricated Persona, T0097.111: Government Official Persona). They may also impersonate existing members of government (T0143.003: Impersonated Persona, T0097.111: Government Official Persona).\n\n Legitimate government officials could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.111: Government Official Persona). For example, a government official could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.112",
              "name": "Government Employee Persona",
              "tactic_id": "TA16",
              "summary": "A person who presents as an active or previous civil servant has the government employee persona. These are professionals hired to serve in government institutions and departments, not officials selected to represent constituents, or assigned official roles in government (such as heads of departments).\n\n Presenting as a government employee is not an indication of inauthentic behaviour, however threat actors may fabricate individuals who work in government to add credibility to their narratives (T0143.002: Fabricated Persona, T0097.112: Government Employee Persona). They may also impersonate existing government employees (T0143.003: Impersonated Persona, T0097.112: Government Employee Persona).\n\n Legitimate government employees could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.112: Government Employee Persona). For example, a government employee could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.200",
              "name": "Institutional Persona",
              "tactic_id": "TA16",
              "summary": "This Technique can be used to indicate that an entity is presenting itself as an institution. If the organisation is presenting itself as having one of the personas listed below then these Techniques should be used instead, as they indicate both that the entity presented itself as an institution, and the type of persona they presented:\n\n T0097.201: Local Institution Persona\n T0097.202: News Outlet Persona\n T0097.203: Fact Checking Organisation Persona\n T0097.204: Think Tank Persona\n T0097.205: Business Persona\n T0097.206: Government Institution Persona\n T0097.207: NGO Persona\n T0097.208: Social Cause Persona",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.201",
              "name": "Local Institution Persona",
              "tactic_id": "TA16",
              "summary": "Institutions which present themselves as operating in a particular geography, or as having local knowledge relevant to a narrative, are presenting a local institution persona.\n\n While presenting as a local institution is not an indication of inauthentic behaviour, threat actors may present themselves as such (T0143.002: Fabricated Persona, T0097.201: Local Institution Persona) to add credibility to their narratives, or misrepresent the real opinions of locals in the area.\n\n Legitimate local institutions could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.201: Local Institution Persona). For example, a local institution could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.202",
              "name": "News Outlet Persona",
              "tactic_id": "TA16",
              "summary": "An institution with a news outlet persona presents itself as an organisation which delivers new information to its target audience.\n\n While presenting as a news outlet is not an indication of inauthentic behaviour, an influence operation may have its narratives amplified by news organisations. Threat actors can fabricate news organisations (T0143.002: Fabricated Persona, T0097.202: News Outlet Persona), or they can impersonate existing news outlets (T0143.003: Impersonated Persona, T0097.202: News Outlet Persona).\n\n Legitimate news organisations could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.202: News Outlet Persona).",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.203",
              "name": "Fact Checking Organisation Persona",
              "tactic_id": "TA16",
              "summary": "An institution with a fact checking organisation persona presents itself as an organisation which produces reports which assess the validity of others’ reporting / statements.\n\n While presenting as a fact checking organisation is not an indication of inauthentic behaviour, an influence operation may have its narratives amplified by fact checking organisations. Threat actors can fabricate fact checking organisations (T0143.002: Fabricated Persona, T0097.202: News Outlet Persona), or they can impersonate existing fact checking outlets (T0143.003: Impersonated Persona, T0097.202: News Outlet Persona).\n\n Legitimate fact checking organisations could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.202: News Outlet Persona).",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.204",
              "name": "Think Tank Persona",
              "tactic_id": "TA16",
              "summary": "An institution with a think tank persona presents itself as a think tank; an organisation that aims to conduct original research and propose new policies or solutions, especially for social and scientific problems.\n\n While presenting as a think tank is not an indication of inauthentic behaviour, think tank personas are commonly used by threat actors as a front for their operational activity (T0143.002: Fabricated Persona, T0097.204: Think Tank Persona). They may be created to give legitimacy to narratives and allow them to suggest politically beneficial solutions to societal issues.\n\n Legitimate think tanks could have a political bias that they may not be transparent about, they could use their persona for malicious purposes, or they could be exploited by threat actors (T0143.001: Authentic Persona, T0097.204: Think Tank Persona). For example, a think tank could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.205",
              "name": "Business Persona",
              "tactic_id": "TA16",
              "summary": "An institution with a business persona presents itself as a for-profit organisation which provides goods or services for a price.\n\n While presenting as a business is not an indication of inauthentic behaviour, business personas may be used by threat actors as a front for their operational activity (T0143.002: Fabricated Persona, T0097.205: Business Persona).\n\n Threat actors may also impersonate existing businesses (T0143.003: Impersonated Persona, T0097.205: Business Persona) to exploit their brand or cause reputational damage.\n\n Legitimate businesses could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.205: Business Persona). For example, a business could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.206",
              "name": "Government Institution Persona",
              "tactic_id": "TA16",
              "summary": "Institutions which present themselves as governments, or government ministries, are presenting a government institution persona.\n\n While presenting as a government institution is not an indication of inauthentic behaviour, threat actors may impersonate existing government institutions as part of their operation (T0143.003: Impersonated Persona, T0097.206: Government Institution Persona), to add legitimacy to their narratives, or discredit the government.\n\n Legitimate government institutions could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.206: Government Institution Persona). For example, a government institution could be used by elected officials to spread inauthentic narratives.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.207",
              "name": "NGO Persona",
              "tactic_id": "TA16",
              "summary": "Institutions which present themselves as an NGO (Non-Governmental Organisation), an organisation which provides services or advocates for public policy (while not being directly affiliated with any government), are presenting an NGO persona.\n\n While presenting as an NGO is not an indication of inauthentic behaviour, NGO personas are commonly used by threat actors (such as intelligence services) as a front for their operational activity (T0143.002: Fabricated Persona, T0097.207: NGO Persona). They are created to give legitimacy to the influence operation and potentially infiltrate grassroots movements\n\n Legitimate NGOs could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.207: NGO Persona). For example, an NGO could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            },
            {
              "disarm_id": "T0097.208",
              "name": "Social Cause Persona",
              "tactic_id": "TA16",
              "summary": "Online accounts which present themselves as focusing on a social cause are presenting the Social Cause Persona. Examples include accounts which post about current affairs, such as discrimination faced by minorities.\n\n While presenting as an account invested in a social cause is not an indication of inauthentic behaviour, such personas have been used by threat actors to exploit peoples’ legitimate emotional investment regarding social causes that matter to them (T0143.002: Fabricated Persona, T0097.208: Social Cause Persona).\n\n Legitimate accounts focused on a social cause could use their persona for malicious purposes, or be exploited by threat actors (T0143.001: Authentic Persona, T0097.208: Social Cause Persona). For example, the account holders could take money for using their position to provide legitimacy to a false narrative, or be tricked into doing so without their knowledge.",
              "changes_from_v0_1": "New in V1.5",
              "long_name": ""
            }
          ]
        }
      ],
      "actors_and_groups": [
        {
          "group_id": "G0001",
          "group_name": "Internet Research Agency (IRA)",
          "aliases": "Troll farm, Glavset",
          "country": "Russia",
          "description": "A Russian company engaged in online influence operations on behalf of the Russian government. The IRA is known for its use of fake accounts and social media to spread disinformation and propaganda.",
          "attribution": "US intelligence community, social media platforms"
        },
        {
          "group_id": "G0002",
          "group_name": "QAnon",
          "aliases": null,
          "country": "United States",
          "description": "A decentralized, far-right conspiracy theory and movement. QAnon followers believe that a cabal of Satan-worshipping pedophiles is running a global child sex-trafficking ring and that former President Donald Trump is secretly fighting against them.",
          "attribution": null
        }
      ],
      "known_incidents": [
        {
          "incident_id": "I0001",
          "incident_name": "Russian Interference in 2016 US Election",
          "description": "A multi-faceted influence operation conducted by the Russian government to interfere in the 2016 US presidential election. The operation involved the use of social media, state-sponsored media, and hack-and-leak operations to spread disinformation and sow discord.",
          "start_date": "2014-01-01T00:00:00",
          "end_date": "2017-01-20T00:00:00",
          "attributed_to": "G0001"
        },
        {
          "incident_id": "I0002",
          "incident_name": "COVID-19 Disinformation",
          "description": "A global wave of disinformation and misinformation related to the COVID-19 pandemic. The narratives included false claims about the origin of the virus, ineffective or harmful treatments, and conspiracy theories about vaccines. A wide range of state and non-state actors were involved.",
          "start_date": "2019-12-01T00:00:00",
          "end_date": null,
          "attributed_to": null
        }
      ],
      "tools_and_software": [
        {
          "software_id": "S0001",
          "software_name": "Social Media Bots",
          "description": "Automated social media accounts that are used to amplify disinformation and create the illusion of widespread support for a particular narrative. Bots can be used to like, share, and comment on posts, as well as to follow and message other users.",
          "type": "Bot"
        },
        {
          "software_id": "S0002",
          "software_name": "Deepfake Software",
          "description": "Software that uses artificial intelligence to create realistic-looking fake videos or audio recordings. Deepfakes can be used to create false evidence of events that never happened, or to put words into the mouths of public figures.",
          "type": "Audiovisual manipulation"
        }
      ],
      "mitigations": [
        {
          "mitigation_id": "M0001",
          "mitigation_name": "Disinformation awareness and education",
          "description": "Initiatives to educate the public about the threat of disinformation and to provide them with the skills to identify and debunk false narratives. This can include media literacy training, public awareness campaigns, and educational materials.",
          "phase": "Preparation"
        },
        {
          "mitigation_id": "M0002",
          "mitigation_name": "Prebunking and inoculation",
          "description": "A proactive approach to countering disinformation by exposing people to weakened forms of false narratives before they are exposed to the real thing. This can help to build up psychological resistance to disinformation.",
          "phase": "Preparation"
        },
        {
          "mitigation_id": "M0003",
          "mitigation_name": "Debunking",
          "description": "The process of refuting false narratives after they have already been spread. This can involve fact-checking, providing counter-evidence, and explaining how the disinformation is misleading.",
          "phase": "Response"
        }
      ]
    }
  }